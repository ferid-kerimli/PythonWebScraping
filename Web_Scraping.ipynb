{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "scraped_df = pd.DataFrame(columns=['Link', 'Price', 'Date', 'Text',\n",
    "                                   'Location', 'Area', 'Rooms', 'Located Floor',\n",
    "                                   'Number of Floors', 'Repair', 'Document'])\n",
    "\n",
    "base_url = 'https://www.emlak.az/elanlar/?ann_type=3&announce_type=18880&property_type=1&room_min=1&room_max=5&sort_type=0&page='\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Mobile Safari/537.36'}\n",
    "\n",
    "page_num = 1\n",
    "max_retries = 3\n",
    "\n",
    "while page_num<200: #True:\n",
    "    print(f'Scraping page {page_num}...')\n",
    "    url = f'{base_url}{page_num}'\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = requests.get(url, headers=header)\n",
    "\n",
    "            if r.status_code != 200:\n",
    "                print(f\"Failed to retrieve page {page_num}. Status code: {r.status_code}. Attempt {attempt+1} of {max_retries}\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(r.content, 'lxml')\n",
    "            ticket_list = soup.find('div', class_='ticket-list')\n",
    "\n",
    "            if ticket_list is None or not ticket_list.find_all('div'):\n",
    "                print(f'No ticket-list or inner divs found on page {page_num}. Stopping scraping.')\n",
    "                break\n",
    "\n",
    "            div_class = ' '.join(ticket_list.find('div').get('class', []))\n",
    "\n",
    "            houses = ticket_list.find_all('div', class_=f\"{div_class}\")\n",
    "            if not houses:\n",
    "                print('No more data found in the houses divs. Ending scraping.')\n",
    "                break\n",
    "\n",
    "            for house in houses:\n",
    "                house_links = house.find_all('div', attrs={'class': 'img'})\n",
    "                for link in house_links:\n",
    "                    a_tag = link.find('a')\n",
    "                    link_basi = 'https://emlak.az'\n",
    "                    if a_tag and 'href' in a_tag.attrs:\n",
    "                        full_link = link_basi + a_tag['href']\n",
    "                        detail_attempts = 0\n",
    "                        while detail_attempts < max_retries:\n",
    "                            try:\n",
    "                                detail = requests.get(full_link, headers=header)\n",
    "                                \n",
    "                                if detail.status_code != 200:\n",
    "                                    print(f\"Failed to retrieve detail page {full_link}. Status code: {detail.status_code}. Attempt {detail_attempts+1} of {max_retries}\")\n",
    "                                    detail_attempts += 1\n",
    "                                    time.sleep(5)\n",
    "                                    continue\n",
    "                                \n",
    "                                detail_soup = BeautifulSoup(detail.content, 'lxml')\n",
    "                                \n",
    "                                prices = [price.get_text(strip=True) for price in detail_soup.find_all('span', attrs={'class': 'm'})]\n",
    "                                \n",
    "                                strong_tags = detail_soup.find_all('strong')\n",
    "                                date = strong_tags[1].get_text(strip=True) if len(strong_tags) > 1 else None\n",
    "                                \n",
    "                                div_text = detail_soup.find_all('div', class_='desc')\n",
    "                                text = [p.get_text(strip=True) for div in div_text for p in div.find_all('p')]\n",
    "                                \n",
    "                                location_div = detail_soup.find_all('div', class_='map-address')\n",
    "                                location = [h4.get_text(strip=True) for div in location_div for h4 in div.find_all('h4')]\n",
    "                                \n",
    "                                area = None\n",
    "                                rooms = None\n",
    "                                located_floor = None\n",
    "                                number_of_floors = None\n",
    "                                repair = None\n",
    "                                document = None\n",
    "\n",
    "                                for dd in detail_soup.find_all('dd'):\n",
    "                                    if 'Sahə' in dd.text:\n",
    "                                        area = dd.text.replace('Sahə', '').strip()\n",
    "                                    if 'Otaqların sayı' in dd.text:\n",
    "                                        rooms = dd.text.replace('Otaqların sayı', '').strip()\n",
    "                                    if 'Yerləşdiyi mərtəbə' in dd.text:\n",
    "                                        located_floor = dd.text.replace('Yerləşdiyi mərtəbə', '').strip()\n",
    "                                    if 'Mərtəbə sayı' in dd.text:\n",
    "                                        number_of_floors = dd.text.replace('Mərtəbə sayı', '').strip()\n",
    "                                    if 'Təmiri' in dd.text:\n",
    "                                        repair = dd.text.replace('Təmiri', '').strip()\n",
    "                                    if 'Sənədin tipi' in dd.text:\n",
    "                                        document = dd.text.replace('Sənədin tipi', '').strip()\n",
    "                                \n",
    "                                current_data = pd.DataFrame([{\n",
    "                                    'Link': full_link,\n",
    "                                    'Price': prices,\n",
    "                                    'Date': date,\n",
    "                                    'Text': text,\n",
    "                                    'Area': area,\n",
    "                                    'Document': document,\n",
    "                                    'Location': location,\n",
    "                                    'Rooms': rooms,\n",
    "                                    'Located Floor': located_floor,\n",
    "                                    'Number of Floors': number_of_floors,\n",
    "                                    'Repair': repair\n",
    "                                }])\n",
    "\n",
    "                                scraped_df = pd.concat([scraped_df, current_data], ignore_index=True)\n",
    "                                break\n",
    "                            \n",
    "                            except Exception as e:\n",
    "                                print(f\"An error occurred when scraping detail page {full_link}: {e}. Attempt {detail_attempts+1} of {max_retries}\")\n",
    "                                detail_attempts += 1\n",
    "                                time.sleep(5)\n",
    "                        if detail_attempts == max_retries:\n",
    "                            print(f\"Failed to retrieve detail page {full_link} after {max_retries} attempts. Skipping.\")\n",
    "                            \n",
    "            page_num += 1\n",
    "            time.sleep(1)\n",
    "            break  \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred on page {page_num}: {e}. Attempt {attempt+1} of {max_retries}\")\n",
    "            time.sleep(5) \n",
    "            \n",
    "    else:\n",
    "        print(f\"Failed to retrieve page {page_num} after {max_retries} attempts. Skipping to next page.\")\n",
    "        page_num += 1\n",
    "\n",
    "    if ticket_list is None or not ticket_list.find_all('div'):\n",
    "        break\n",
    "\n",
    "scraped_df.to_csv('scraped_data_team_11.csv', index=False)\n",
    "                                "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
